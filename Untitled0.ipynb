{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EldKJVz4xbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVzaDRXX4052",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "04939427-0f2f-41ef-f880-3a75e27ff8e9"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWKtKMlK5BJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "249a9efc-b1ae-489e-9c2b-366e7744fd3b"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text=\"\"\"Hi Learners, hope you are all good. In your project there is an important step which you need to understand, which is called tokenization. What is Tokenization? Well, Tokenization is the first step in Text Analytics. So, when we need to find grammatical errors in text or semantic errors or perform emotion and sentiment analysis of different textual comments and feedback, we would need to break down huge amount of textual data into subsequently smaller chunks, till the textual context is identified for proper analysis and obtaining insight from it. On a big picture, this is a step towards Text Mining. \"\"\"\n",
        "\n",
        "tokenized_text=sent_tokenize(text)\n",
        "\n",
        "print(tokenized_text)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi Learners, hope you are all good.', 'In your project there is an important step which you need to understand, which is called tokenization.', 'What is Tokenization?', 'Well, Tokenization is the first step in Text Analytics.', 'So, when we need to find grammatical errors in text or semantic errors or perform emotion and sentiment analysis of different textual comments and feedback, we would need to break down huge amount of textual data into subsequently smaller chunks, till the textual context is identified for proper analysis and obtaining insight from it.', 'On a big picture, this is a step towards Text Mining.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwlQUmGq6QBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a6f7f2f8-9e33-4f06-cd9a-59ee4792aed8"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text=\"\"\"Hi Learners hope you are all good. In your project there is an important step which you need to understand, the process of tokenization. What is Tokenization? Tokenization is the first step in Text Analytics. When we need to find grammatical errors in text or semantic errors or perform emotion and sentiment analysis of different textual comments and feedback we would need to break down huge amount of textual data into subsequently smaller chunks, till the textual context is identified for proper analysis and obtaining insight from it. On a big picture, this is a step towards Text Mining. \"\"\"\n",
        "tokenized_text=sent_tokenize(text)\n",
        "print('The Tokenized statements are : ',tokenized_text)\n",
        "tokenized_word=word_tokenize(text)\n",
        "print('The Tokenized Words are : ', tokenized_word)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Tokenized statements are :  ['Hi Learners hope you are all good.', 'In your project there is an important step which you need to understand, the process of tokenization.', 'What is Tokenization?', 'Tokenization is the first step in Text Analytics.', 'When we need to find grammatical errors in text or semantic errors or perform emotion and sentiment analysis of different textual comments and feedback we would need to break down huge amount of textual data into subsequently smaller chunks, till the textual context is identified for proper analysis and obtaining insight from it.', 'On a big picture, this is a step towards Text Mining.']\n",
            "The Tokenized Words are :  ['Hi', 'Learners', 'hope', 'you', 'are', 'all', 'good', '.', 'In', 'your', 'project', 'there', 'is', 'an', 'important', 'step', 'which', 'you', 'need', 'to', 'understand', ',', 'the', 'process', 'of', 'tokenization', '.', 'What', 'is', 'Tokenization', '?', 'Tokenization', 'is', 'the', 'first', 'step', 'in', 'Text', 'Analytics', '.', 'When', 'we', 'need', 'to', 'find', 'grammatical', 'errors', 'in', 'text', 'or', 'semantic', 'errors', 'or', 'perform', 'emotion', 'and', 'sentiment', 'analysis', 'of', 'different', 'textual', 'comments', 'and', 'feedback', 'we', 'would', 'need', 'to', 'break', 'down', 'huge', 'amount', 'of', 'textual', 'data', 'into', 'subsequently', 'smaller', 'chunks', ',', 'till', 'the', 'textual', 'context', 'is', 'identified', 'for', 'proper', 'analysis', 'and', 'obtaining', 'insight', 'from', 'it', '.', 'On', 'a', 'big', 'picture', ',', 'this', 'is', 'a', 'step', 'towards', 'Text', 'Mining', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOgkACjv6nO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "11c36dfa-ad69-4f18-c2d9-c8f6b32ed856"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist(tokenized_word)\n",
        "print('Sampling ', fdist)\n",
        "print('The first 3 frequently used tokens are')\n",
        "fdist.most_common(3)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampling  <FreqDist with 75 samples and 108 outcomes>\n",
            "The first 3 frequently used tokens are\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 5), ('is', 5), ('step', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuG_ecxS61Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hRKxurb7KKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}